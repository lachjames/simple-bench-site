{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 4:\n",
      "\n",
      "Chain of Thought:\n",
      "--------------------------------------------------\n",
      "Let me solve this step by step:\n",
      "\n",
      "1) We know one sister always speaks mistruths and the other always lies, but we don't know which is which.\n",
      "\n",
      "2) Let's evaluate each option:\n",
      "- Option E and F are irrelevant to finding the correct path\n",
      "- Option B won't help find the path\n",
      "- Option D is not logically helpful\n",
      "\n",
      "3) Let's analyze Option C \"What path leads to the treasure?\"\n",
      "- If you ask the truth-teller, she'll tell the truth\n",
      "- If you ask the liar, she'll lie\n",
      "- Since we don't know which is which, this won't help\n",
      "\n",
      "4) Let's analyze Option A \"What would your sister say if I asked her which path leads to the treasure?\"\n",
      "- If you ask the truth-teller: She would truthfully tell you what her lying sister would say (which would be the wrong path)\n",
      "- If you ask the liar: She would lie about what her truth-telling sister would say (so she would also indicate the wrong path)\n",
      "\n",
      "5) Key insight: No matter which sister you ask Option A to, they will both indicate the wrong path. Therefore, you just need to take the opposite path of what they say.\n",
      "\n",
      "This question works perfectly because:\n",
      "- It will always give you the wrong answer (regardless of which sister you ask)\n",
      "- You can then choose the opposite path to find the treasure\n",
      "- It's the shortest question that guarantees finding the treasure\n",
      "\n",
      "Final Answer: A\n",
      "--------------------------------------------------\n",
      "Expected: C\n",
      "Got: A\n",
      "Correct: ✗\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "\n",
    "# Lock for thread-safe printing and progress updates\n",
    "print_lock = Lock()\n",
    "results_lock = Lock()\n",
    "\n",
    "def create_client():\n",
    "    return OpenAI(\n",
    "        api_key=\"abcd\",\n",
    "        base_url=\"http://localhost:3060/auto/v1/\",\n",
    "        default_headers={\n",
    "            \"x-proxy-key\": \"3870443e-1f3e-4e5d-a1fe-4e945828cdc6\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "def extract_answer(response: str) -> str:\n",
    "    \"\"\"Extract the final answer (A-F) from the model's response.\"\"\"\n",
    "    match = re.search(r\"Final Answer:\\s*([A-F])\", response, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    match = re.search(r\"([A-F])\\s*$\", response, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    return \"INVALID\"\n",
    "\n",
    "def evaluate_single_question(args):\n",
    "    \"\"\"Worker function for thread pool.\"\"\"\n",
    "    client, question, model, pbar, total_questions = args\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": question[\"prompt\"]}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        model_response = response.choices[0].message.content\n",
    "        model_answer = extract_answer(model_response)\n",
    "        correct = model_answer == question[\"answer\"]\n",
    "        \n",
    "        result = {\n",
    "            \"question_id\": question[\"question_id\"],\n",
    "            \"correct_answer\": question[\"answer\"],\n",
    "            \"model_answer\": model_answer,\n",
    "            \"correct\": correct,\n",
    "            \"full_response\": model_response,\n",
    "            \"prompt\": question[\"prompt\"]\n",
    "        }\n",
    "        \n",
    "        # Thread-safe progress and result printing\n",
    "        with print_lock:\n",
    "            pbar.update(1)\n",
    "            print(f\"\\nQuestion {question['question_id']}:\")\n",
    "            print(\"\\nChain of Thought:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(model_response)\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Expected: {question['answer']}\")\n",
    "            print(f\"Got: {model_answer}\")\n",
    "            print(f\"Correct: {'✓' if correct else '✗'}\")\n",
    "            print(\"=\" * 80)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        with print_lock:\n",
    "            pbar.update(1)\n",
    "            print(f\"\\nError processing question {question['question_id']}: {str(e)}\")\n",
    "        \n",
    "        return {\n",
    "            \"question_id\": question[\"question_id\"],\n",
    "            \"correct_answer\": question[\"answer\"],\n",
    "            \"model_answer\": \"ERROR\",\n",
    "            \"correct\": False,\n",
    "            \"full_response\": str(e),\n",
    "            \"prompt\": question[\"prompt\"]\n",
    "        }\n",
    "\n",
    "def save_results(results: List[Dict], model: str, output_dir: str = \"evaluation_results\"):\n",
    "    \"\"\"Save results in multiple formats with timestamps.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_path = Path(output_dir) / timestamp\n",
    "    base_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    detailed_results = {\n",
    "        \"model\": model,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"results\": results\n",
    "    }\n",
    "    with open(base_path / \"detailed_results.json\", 'w') as f:\n",
    "        json.dump(detailed_results, f, indent=2)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(base_path / \"results.csv\", index=False)\n",
    "    \n",
    "    cot_dir = base_path / \"chain_of_thought\"\n",
    "    cot_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for result in results:\n",
    "        with open(cot_dir / f\"question_{result['question_id']}.txt\", 'w') as f:\n",
    "            f.write(f\"Question ID: {result['question_id']}\\n\")\n",
    "            f.write(f\"Prompt:\\n{result['prompt']}\\n\\n\")\n",
    "            f.write(f\"Model Response:\\n{result['full_response']}\\n\\n\")\n",
    "            f.write(f\"Expected Answer: {result['correct_answer']}\\n\")\n",
    "            f.write(f\"Model Answer: {result['model_answer']}\\n\")\n",
    "            f.write(f\"Correct: {result['correct']}\\n\")\n",
    "\n",
    "def evaluate_model(json_file: str, model: str = \"openai@gpt-4o\", num_threads: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Evaluate the model on all questions in the JSON file using multiple threads.\"\"\"\n",
    "    # Load questions\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    questions = data[\"eval_data\"]\n",
    "    total_questions = len(questions)\n",
    "    results = []\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(total=total_questions, desc=f\"Evaluating {model}\")\n",
    "    \n",
    "    # Create a client for each thread\n",
    "    clients = [create_client() for _ in range(num_threads)]\n",
    "    \n",
    "    # Prepare arguments for thread pool\n",
    "    thread_args = [\n",
    "        (clients[i % num_threads], question, model, pbar, total_questions)\n",
    "        for i, question in enumerate(questions)\n",
    "    ]\n",
    "    \n",
    "    # Process questions in parallel\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        future_to_question = {\n",
    "            executor.submit(evaluate_single_question, args): args[1][\"question_id\"]\n",
    "            for args in thread_args\n",
    "        }\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(future_to_question):\n",
    "            result = future.result()\n",
    "            with results_lock:\n",
    "                results.append(result)\n",
    "    \n",
    "    # Sort results by question_id to maintain order\n",
    "    results.sort(key=lambda x: x[\"question_id\"])\n",
    "    \n",
    "    # Save results\n",
    "    save_results(results, model)\n",
    "    \n",
    "    # Calculate and display final metrics\n",
    "    df = pd.DataFrame(results)\n",
    "    correct_answers = df['correct'].sum()\n",
    "    accuracy = correct_answers / total_questions\n",
    "    \n",
    "    print(f\"\\nFinal Results for {model}\")\n",
    "    print(f\"Total Questions: {total_questions}\")\n",
    "    print(f\"Correct Answers: {correct_answers}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    results_df = evaluate_model(\n",
    "        \"simple_bench_public.json\", \n",
    "        # model=\"openai@gpt-4o-mini\",\n",
    "        model=\"anthropic@claude-3-5-sonnet-latest\",\n",
    "        # num_threads=10,\n",
    "        # model=\"openai@o1-mini\", \n",
    "        # model=\"openai@o1-mini\", \n",
    "        # num_threads=2,\n",
    "        # model=\"gemini@gemini-2.0-flash-exp\",\n",
    "        num_threads=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Dict, Optional, Set\n",
    "from enum import Enum\n",
    "import networkx as nx\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class StepStatus(Enum):\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "class ChainLogger:\n",
    "    \"\"\"Handles logging for model chain execution\"\"\"\n",
    "    def __init__(self, log_dir: str = \"chain_logs\"):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.log_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create run-specific directory\n",
    "        self.run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.run_dir = self.log_dir / self.run_id\n",
    "        self.run_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Setup logging\n",
    "        self.logger = logging.getLogger(f\"chain_run_{self.run_id}\")\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # File handler for detailed logs\n",
    "        fh = logging.FileHandler(self.run_dir / \"detailed.log\")\n",
    "        fh.setLevel(logging.INFO)\n",
    "        \n",
    "        # Console handler for important updates\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.INFO)\n",
    "        \n",
    "        # Formatting\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        fh.setFormatter(formatter)\n",
    "        ch.setFormatter(formatter)\n",
    "        \n",
    "        self.logger.addHandler(fh)\n",
    "        self.logger.addHandler(ch)\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.metrics = {\n",
    "            \"step_times\": {},\n",
    "            \"success_rates\": {},\n",
    "            \"total_questions\": 0,\n",
    "            \"completed_questions\": 0,\n",
    "            \"errors\": [],\n",
    "        }\n",
    "    \n",
    "    def log_step_start(self, step_name: str, question_id: str):\n",
    "        \"\"\"Log the start of a step execution\"\"\"\n",
    "        self.logger.info(f\"Starting step {step_name} for question {question_id}\")\n",
    "        if step_name not in self.metrics[\"step_times\"]:\n",
    "            self.metrics[\"step_times\"][step_name] = []\n",
    "        return time.time()\n",
    "    \n",
    "    def log_step_end(self, step_name: str, question_id: str, start_time: float, success: bool):\n",
    "        \"\"\"Log the completion of a step execution\"\"\"\n",
    "        duration = time.time() - start_time\n",
    "        self.metrics[\"step_times\"][step_name].append(duration)\n",
    "        \n",
    "        if step_name not in self.metrics[\"success_rates\"]:\n",
    "            self.metrics[\"success_rates\"][step_name] = {\"success\": 0, \"total\": 0}\n",
    "        \n",
    "        self.metrics[\"success_rates\"][step_name][\"total\"] += 1\n",
    "        if success:\n",
    "            self.metrics[\"success_rates\"][step_name][\"success\"] += 1\n",
    "        \n",
    "        self.logger.info(\n",
    "            f\"Completed step {step_name} for question {question_id} \"\n",
    "            f\"(duration: {duration:.2f}s, success: {success})\"\n",
    "        )\n",
    "    \n",
    "    def log_error(self, step_name: str, question_id: str, error: Exception):\n",
    "        \"\"\"Log an error during execution\"\"\"\n",
    "        self.logger.error(f\"Error in step {step_name} for question {question_id}: {str(error)}\")\n",
    "        self.metrics[\"errors\"].append({\n",
    "            \"step\": step_name,\n",
    "            \"question\": question_id,\n",
    "            \"error\": str(error),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    def log_response(self, step_name: str, question_id: str, response: Dict):\n",
    "        \"\"\"Log a model response\"\"\"\n",
    "        response_file = self.run_dir / f\"responses_{question_id}.jsonl\"\n",
    "        with open(response_file, \"a\") as f:\n",
    "            f.write(json.dumps({\n",
    "                \"step\": step_name,\n",
    "                \"question\": question_id,\n",
    "                \"response\": response,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }) + \"\\n\")\n",
    "    \n",
    "    def save_metrics(self):\n",
    "        \"\"\"Save current metrics to file\"\"\"\n",
    "        metrics_file = self.run_dir / \"metrics.json\"\n",
    "        \n",
    "        # Calculate averages and success rates\n",
    "        summary = {\n",
    "            \"average_step_times\": {\n",
    "                step: sum(times)/len(times) if times else 0\n",
    "                for step, times in self.metrics[\"step_times\"].items()\n",
    "            },\n",
    "            \"success_rates\": {\n",
    "                step: (data[\"success\"] / data[\"total\"] if data[\"total\"] > 0 else 0)\n",
    "                for step, data in self.metrics[\"success_rates\"].items()\n",
    "            },\n",
    "            \"total_questions\": self.metrics[\"total_questions\"],\n",
    "            \"completed_questions\": self.metrics[\"completed_questions\"],\n",
    "            \"error_count\": len(self.metrics[\"errors\"])\n",
    "        }\n",
    "        \n",
    "        with open(metrics_file, \"w\") as f:\n",
    "            json.dump({\n",
    "                \"raw_metrics\": self.metrics,\n",
    "                \"summary\": summary\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "@dataclass\n",
    "class ModelStep:\n",
    "    \"\"\"Combined model step and result tracking\"\"\"\n",
    "    name: str\n",
    "    model_name: str\n",
    "    system_prompt: str = \"You are a helpful assistant.\"\n",
    "    depends_on: Set[str] = None\n",
    "    status: StepStatus = StepStatus.PENDING\n",
    "    response: Optional[Dict] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.depends_on = self.depends_on or set()\n",
    "\n",
    "class ModelChain:\n",
    "    \"\"\"Model chain with integrated scheduling and logging\"\"\"\n",
    "    def __init__(self, steps: List[ModelStep], max_concurrent: int = 5):\n",
    "        self.steps = {step.name: step for step in steps}\n",
    "        self.max_concurrent = max_concurrent\n",
    "        self.execution_order = self._compute_execution_order()\n",
    "        self.logger = ChainLogger()\n",
    "    \n",
    "    def execute(self, client: Any, question: Dict, pbar: Optional[tqdm] = None) -> List[Dict]:\n",
    "        \"\"\"Execute the model chain with logging\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for step_name in self.execution_order:\n",
    "            step = self.steps[step_name]\n",
    "            \n",
    "            if not self._can_execute(step_name):\n",
    "                continue\n",
    "            \n",
    "            start_time = self.logger.log_step_start(step_name, question[\"question_id\"])\n",
    "            \n",
    "            try:\n",
    "                step.status = StepStatus.RUNNING\n",
    "                dep_responses = self._get_dependency_responses(step_name)\n",
    "                \n",
    "                response = evaluate_single_step(\n",
    "                    client,\n",
    "                    step,\n",
    "                    question[\"prompt\"],\n",
    "                    dep_responses\n",
    "                )\n",
    "                \n",
    "                step.response = response\n",
    "                step.status = StepStatus.COMPLETED\n",
    "                results.append(response)\n",
    "                \n",
    "                self.logger.log_step_end(step_name, question[\"question_id\"], start_time, True)\n",
    "                self.logger.log_response(step_name, question[\"question_id\"], response)\n",
    "                \n",
    "                if pbar:\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                step.status = StepStatus.FAILED\n",
    "                self.logger.log_error(step_name, question[\"question_id\"], e)\n",
    "                self.logger.log_step_end(step_name, question[\"question_id\"], start_time, False)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _compute_execution_order(self) -> List[str]:\n",
    "        \"\"\"Compute topological sort of steps\"\"\"\n",
    "        graph = nx.DiGraph()\n",
    "        for step in self.steps.values():\n",
    "            graph.add_node(step.name)\n",
    "            for dep in step.depends_on:\n",
    "                if dep not in self.steps:\n",
    "                    raise ValueError(f\"Unknown dependency: {dep}\")\n",
    "                graph.add_edge(dep, step.name)\n",
    "        \n",
    "        if not nx.is_directed_acyclic_graph(graph):\n",
    "            raise ValueError(\"Dependency cycle detected\")\n",
    "        \n",
    "        return list(nx.topological_sort(graph))\n",
    "    \n",
    "    def _can_execute(self, step_name: str) -> bool:\n",
    "        \"\"\"Check if all dependencies are completed\"\"\"\n",
    "        step = self.steps[step_name]\n",
    "        return all(\n",
    "            self.steps[dep].status == StepStatus.COMPLETED\n",
    "            for dep in step.depends_on\n",
    "        )\n",
    "    \n",
    "    def _get_dependency_responses(self, step_name: str) -> List[Dict]:\n",
    "        \"\"\"Get responses from completed dependencies\"\"\"\n",
    "        step = self.steps[step_name]\n",
    "        return [\n",
    "            self.steps[dep].response\n",
    "            for dep in step.depends_on\n",
    "            if self.steps[dep].response\n",
    "        ]\n",
    "    \n",
    "def evaluate_model_chain(json_file: str, model_chain: ModelChain, num_threads: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Evaluate using parallel execution of questions with comprehensive logging\"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        questions = json.load(f)[\"eval_data\"]\n",
    "    \n",
    "    model_chain.logger.metrics[\"total_questions\"] = len(questions)\n",
    "    total_steps = len(model_chain.steps) * len(questions)\n",
    "    pbar = tqdm(total=total_steps, desc=\"Evaluating\")\n",
    "    \n",
    "    results = []\n",
    "    clients = [create_client() for _ in range(num_threads)]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [\n",
    "            executor.submit(model_chain.execute, clients[i % num_threads], q, pbar)\n",
    "            for i, q in enumerate(questions)\n",
    "        ]\n",
    "        \n",
    "        for future, question in zip(futures, questions):\n",
    "            try:\n",
    "                step_responses = future.result()\n",
    "                final_answer = step_responses[-1][\"extracted_answer\"]\n",
    "                correct = final_answer == question[\"answer\"]\n",
    "                \n",
    "                result = {\n",
    "                    \"question_id\": question[\"question_id\"],\n",
    "                    \"correct_answer\": question[\"answer\"],\n",
    "                    \"model_answer\": final_answer,\n",
    "                    \"correct\": correct,\n",
    "                    \"step_responses\": step_responses,\n",
    "                    \"prompt\": question[\"prompt\"]\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                model_chain.logger.metrics[\"completed_questions\"] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                model_chain.logger.log_error(\"evaluation\", question[\"question_id\"], e)\n",
    "                results.append({\n",
    "                    \"question_id\": question[\"question_id\"],\n",
    "                    \"correct_answer\": question[\"answer\"],\n",
    "                    \"model_answer\": \"ERROR\",\n",
    "                    \"correct\": False,\n",
    "                    \"step_responses\": [],\n",
    "                    \"prompt\": question[\"prompt\"]\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save and display final metrics\n",
    "    summary = model_chain.logger.save_metrics()\n",
    "    print(\"\\nEvaluation Summary:\")\n",
    "    print(f\"Total Questions: {summary['total_questions']}\")\n",
    "    print(f\"Completed Questions: {summary['completed_questions']}\")\n",
    "    print(f\"Accuracy: {df['correct'].mean():.2%}\")\n",
    "    print(\"\\nAverage Step Times:\")\n",
    "    for step, time in summary['average_step_times'].items():\n",
    "        print(f\"  {step}: {time:.2f}s\")\n",
    "    print(\"\\nSuccess Rates:\")\n",
    "    for step, rate in summary['success_rates'].items():\n",
    "        print(f\"  {step}: {rate:.2%}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def evaluate_single_step(client: Any, step: ModelStep, prompt: str, \n",
    "                        previous_responses: Optional[List[Dict]] = None) -> Dict:\n",
    "    \"\"\"Execute a single model step\"\"\"\n",
    "    if previous_responses:\n",
    "        context = \"\\n\\nPrevious model responses:\\n\"\n",
    "        for idx, resp in enumerate(previous_responses, 1):\n",
    "            context += f\"\\nModel {idx} ({resp['model']}):\\n{resp['response']}\\n\"\n",
    "            context += f\"Model {idx}'s answer: {resp['extracted_answer']}\\n\"\n",
    "        prompt += context\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=step.model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": step.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    model_response = response.choices[0].message.content\n",
    "    return {\n",
    "        \"model\": step.model_name,\n",
    "        \"response\": model_response,\n",
    "        \"extracted_answer\": extract_answer(model_response)\n",
    "    }\n",
    "\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ModelStep(\n",
    "        name=\"gpt4\",\n",
    "        model_name=\"openai@gpt-4o\",\n",
    "        system_prompt=\"You are an expert at solving multiple choice questions.\"\n",
    "    ),\n",
    "    ModelStep(\n",
    "        name=\"gpt4mini\",\n",
    "        model_name=\"openai@gpt-4o-mini\",\n",
    "        system_prompt=\"You are an expert at solving multiple choice questions.\"\n",
    "    ),\n",
    "    ModelStep(\n",
    "        name=\"claude\",\n",
    "        model_name=\"anthropic@claude-3-5-sonnet-latest\",\n",
    "        system_prompt=\"You are an expert at solving multiple choice questions.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define reviewer and aggregator\n",
    "reviewer = ModelStep(\n",
    "    name=\"reviewer\",\n",
    "    model_name=\"openai@gpt-4o\",\n",
    "    system_prompt=\"Review the responses from GPT-4 and Claude and provide your analysis.\",\n",
    "    depends_on={\"gpt4\", \"claude\"}\n",
    ")\n",
    "\n",
    "aggregator = ModelStep(\n",
    "    name=\"aggregator\",\n",
    "    model_name=\"openai@gpt-4o\",\n",
    "    system_prompt=\"Analyze all previous responses and provide the final answer.\",\n",
    "    depends_on={\"gpt4\", \"gpt4mini\", \"claude\", \"reviewer\"}\n",
    ")\n",
    "\n",
    "# Create and run the chain\n",
    "model_chain = ModelChain(base_models + [reviewer, aggregator])\n",
    "results_df = evaluate_model_chain(\"simple_bench_public.json\", model_chain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
